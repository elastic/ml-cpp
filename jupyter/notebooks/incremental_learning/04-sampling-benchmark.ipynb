{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e2a279",
   "metadata": {},
   "source": [
    "# Benchmarking of different sampling algorithms\n",
    "\n",
    "This notebooks assesses the performance of different sampling algorithms available in Python. The task is to sample `k` items from a csv file stored on the disk. As a baseline, we compare to the speed of reading/parsing a csv file from the disk (without sampling from it).\n",
    "\n",
    "We record the execution time as well as the bandwith in MB/sec.\n",
    "\n",
    "We use `csv`, `pandas` and `dask` to read and parse the csv files. Using `dask` is advisable since it implements the parallelisation concepts similar to those in `elasticsearch` and hence the sampling algorithms implemented on top of `dask` will be easier to port later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea9537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "from incremental_learning.storage import download_dataset\n",
    "from incremental_learning.misc import DataStream, reservoir_sample_with_jumps\n",
    "from incremental_learning.config import datasets_dir\n",
    "\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import time\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1f03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'autompg'\n",
    "dataset_name = 'ember_malware_bytes'\n",
    "dataset = datasets_dir / \"{}.csv\".format(dataset_name)\n",
    "size_mb = dataset.stat().st_size/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce8345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandwidth(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        res = func(*args, **kwargs)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        bandwidth = size_mb/elapsed_time\n",
    "        print(\"Elapsed time: {} sec\\nBandwidth: {} MB/sec\" .format(elapsed_time, bandwidth))\n",
    "        return res\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36827664",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bandwidth\n",
    "def dictreader():\n",
    "    \"\"\"Read and parse the csv file using csv library.\"\"\"\n",
    "    with open(dataset, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile, dialect='unix')\n",
    "        for row in reader:\n",
    "            continue\n",
    "            \n",
    "@bandwidth\n",
    "def csvreader():\n",
    "    \"\"\"Read only the csv file using the csv librar.\"\"\"\n",
    "    with open(dataset, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, dialect='unix')\n",
    "        for row in reader:\n",
    "            continue\n",
    "            \n",
    "@bandwidth\n",
    "def pdreader():\n",
    "    \"\"\"Read the csv file using pandas library.\"\"\"\n",
    "    df = pd.read_csv(dataset)\n",
    "\n",
    "@bandwidth\n",
    "def reservoir(k):\n",
    "    \"\"\"Read from file and sample using reservoir sample la\"\"\"\n",
    "    D = reservoir_sample_with_jumps(dataset, k)\n",
    "    return D\n",
    "    \n",
    "@bandwidth\n",
    "def pdsampler(k):\n",
    "    \"\"\"Read the csv file with panda and sample randomly.\"\"\"\n",
    "    df = pd.read_csv(dataset)\n",
    "    return df.sample(frac=k)\n",
    "\n",
    "@bandwidth\n",
    "def ddsampler(k):\n",
    "    \"\"\"Read the csv file with dusk and sample randomly.\"\"\"\n",
    "    df = dd.read_csv(dataset)\n",
    "    return df.sample(frac=k).compute()\n",
    "\n",
    "@bandwidth\n",
    "def ddsortchoose(k):\n",
    "    \"\"\"Read the csv file with dusk, sort it and choose k maximum elements.\"\"\"\n",
    "    df = dd.read_csv(dataset)\n",
    "    df = df.set_index('byte_histogram_0')\n",
    "    return df.tail(k)\n",
    "\n",
    "@bandwidth\n",
    "def ddnlargest(k):\n",
    "    \"\"\"Read the csv file with dusk and select k largest elements.\"\"\"\n",
    "    df = dd.read_csv(dataset)\n",
    "    return df.nlargest(n=k, columns=['byte_histogram_0']).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356781ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 40.47589683532715 sec\n",
      "Bandwidth: 76.29892688511258 MB/sec\n"
     ]
    }
   ],
   "source": [
    "dictreader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fd9308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 25.200711488723755 sec\n",
      "Bandwidth: 122.54683740297784 MB/sec\n"
     ]
    }
   ],
   "source": [
    "csvreader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd28ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 24.42076063156128 sec\n",
      "Bandwidth: 126.46074132747214 MB/sec\n"
     ]
    }
   ],
   "source": [
    "pdreader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c1e708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 46.38114285469055 sec\n",
      "Bandwidth: 66.58454930537934 MB/sec\n"
     ]
    }
   ],
   "source": [
    "_ = reservoir(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "611f8899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 23.54236626625061 sec\n",
      "Bandwidth: 131.17914564413186 MB/sec\n"
     ]
    }
   ],
   "source": [
    "_ = pdsampler(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9468d48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 10.396746158599854 sec\n",
      "Bandwidth: 297.0417326860934 MB/sec\n"
     ]
    }
   ],
   "source": [
    "_ = ddsampler(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6eabb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 24.716933250427246 sec\n",
      "Bandwidth: 124.94541543476488 MB/sec\n"
     ]
    }
   ],
   "source": [
    "_ = ddsortchoose(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efadba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 11.367468357086182 sec\n",
      "Bandwidth: 271.67592609332763 MB/sec\n"
     ]
    }
   ],
   "source": [
    "_ = ddnlargest(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359520f4",
   "metadata": {},
   "source": [
    "Using the `dask` library brings us close to the bandwidth of simply reading a file onto and SSD drive! `ddsampler` processes the `ember` dataset in about 10 seconds, however, it is unable to do any weighted sampling. With `ddnlargest` we process `ember` in 11 seconds and it is only slightly slower than `ddsampler`, while it is able to use a column of weights to select samples with the highest indicator.\n",
    "\n",
    "As a downside, `ddnlargest` can sample clustered points. As a remedy, we could run `diversipy` psa sampler on top of the reduced sample (e.g. having 5 `k` samples)  to increase the sample diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee6c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
