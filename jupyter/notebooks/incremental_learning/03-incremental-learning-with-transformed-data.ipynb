{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "# Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n",
    "# or more contributor license agreements. Licensed under the Elastic License\n",
    "# 2.0 and the following additional limitation. Functionality enabled by the\n",
    "# files subject to the Elastic License 2.0 may only be used in production when\n",
    "# invoked by an Elasticsearch process with a license key installed that permits\n",
    "# use of machine learning features. You may not use this file except in\n",
    "# compliance with the Elastic License 2.0 and the foregoing additional\n",
    "# limitation.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Incremental Learning 3: Benchmark incremental learning algorithm with transformed data\n",
    "\n",
    "<img src=\"03-incremental-learning-with-transformed-data/3-schema.png\" />\n",
    "\n",
    "In this notebook, we illustrate the evaluation of the performance of the incremental learning algorithm using real-world data transformed according to a predefined transformation operation. The possible transformation operations are:\n",
    "* `partition_on_metric_ranges`: Partition the data frame into values rows not contained and contained in\n",
    "random intervals of metric features.\n",
    "* `partition_on_categories`: Partition the data frame into values rows matching and not matching a random\n",
    "subset of categories.\n",
    "* `resample_metric_features`: Resample by randomly weighting equally spaced quantile buckets of features.\n",
    "* `shift_metric_features`: Apply a random shift to the metric features in the dataset.\n",
    "* `rotate_metric_features`: Downsample and apply a random rotation to the metric feature values in the dataset.\n",
    "* `regression_category_drift`: Downsample and apply a random shift to the target variable for each distinct\n",
    "    category of the categorical_features feature values in dataset.\n",
    "\n",
    "We split the original dataset into two parts: train and test1. We transform both these parts using the specified transformation operation defined in the variable `config`. The transformation of the dataset part `train_dataset` results in `update_dataset`, while the transformation of the dataset part `test1` results in `test2`.\n",
    "\n",
    "Now, we combine the data from `train` and `update` to obtain the `baseline_dataset` and `test1` and `test2` to obtain the `test_dataset`.\n",
    "The `baseline_dataset` is used to train the `baseline_model` from scratch. This model is the \"golden standard\" with which we compare all our other models.\n",
    "\n",
    "We train the `train_model` on the `trained_dataset` without showing it any transformed data. Next, we update this model using `update_dataset` and obtain the `updated_model`.\n",
    "\n",
    "To evaluate the `baseline_model`, `train_model`, and `updated_model`, we use the `test_dataset`. The goal is that the generalization errors from `trained_model` and `updated_model` are as close as possible to our \"golden standard\" of the `baseline_model.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import pprint\n",
    "\n",
    "from incremental_learning.config import jobs_dir, logger\n",
    "from incremental_learning.job import train, update, evaluate\n",
    "from incremental_learning.storage import read_dataset, upload_job, delete_job\n",
    "from incremental_learning.transforms import transform_dataset"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def compute_regression_metrics(y_true,\n",
    "                               baseline_model_predictions,\n",
    "                               trained_model_predictions,\n",
    "                               updated_model_predictions):\n",
    "    scores = {\n",
    "        'baseline': {\n",
    "            'mae': metrics.mean_absolute_error(y_true, baseline_model_predictions),\n",
    "            'mse': metrics.mean_squared_error(y_true, baseline_model_predictions)\n",
    "        },\n",
    "        'trained_model': {\n",
    "            'mae': metrics.mean_absolute_error(y_true, trained_model_predictions),\n",
    "            'mse': metrics.mean_squared_error(y_true, trained_model_predictions)\n",
    "        },\n",
    "        'updated_model': {\n",
    "            'mae': metrics.mean_absolute_error(y_true, updated_model_predictions),\n",
    "            'mse': metrics.mean_squared_error(y_true, updated_model_predictions)\n",
    "        },\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "\n",
    "def compute_classification_metrics(y_true,\n",
    "                                   baseline_model_predictions,\n",
    "                                   trained_model_predictions,\n",
    "                                   updated_model_predictions):\n",
    "    scores = {\n",
    "        'baseline': {\n",
    "            'acc': metrics.accuracy_score(y_true, baseline_model_predictions)\n",
    "        },\n",
    "        'trained_model': {\n",
    "            'acc': metrics.accuracy_score(y_true, trained_model_predictions)\n",
    "        },\n",
    "        'updated_model': {\n",
    "            'acc': metrics.accuracy_score(y_true, updated_model_predictions)\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for label in np.unique(y_true):\n",
    "        scores['baseline']['precision_' + label] = \\\n",
    "            metrics.precision_score(y_true, baseline_model_predictions, pos_label=label)\n",
    "        scores['trained_model']['precision_' + label] = \\\n",
    "            metrics.precision_score(y_true, trained_model_predictions, pos_label=label)\n",
    "        scores['updated_model']['precision_' + label] = \\\n",
    "            metrics.precision_score(y_true, updated_model_predictions, pos_label=label)\n",
    "        scores['baseline']['recall_' + label] = \\\n",
    "            metrics.recall_score(y_true, baseline_model_predictions, pos_label=label)\n",
    "        scores['trained_model']['recall_' + label] = \\\n",
    "            metrics.recall_score(y_true, trained_model_predictions, pos_label=label)\n",
    "        scores['updated_model']['recall_' + label] = \\\n",
    "            metrics.recall_score(y_true, updated_model_predictions, pos_label=label)\n",
    "\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "test_fraction = 0.2\n",
    "config = {\n",
    "    \"dataset_name\": \"ccpp\",\n",
    "    \"seed\": 90982247,\n",
    "    \"threads\": 1,\n",
    "    \"transform_name\": \"partition_on_metric_ranges\",\n",
    "    \"transform_parameters\": {\n",
    "        \"fraction\": 0.45,\n",
    "        \"metric_features\": [\n",
    "                    \"AT\",\n",
    "                    \"AP\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "dataset_name = config['dataset_name']\n",
    "verbose=False\n",
    "force_update = False\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "original_dataset = read_dataset(config['dataset_name'])\n",
    "original_dataset = original_dataset.sample(frac=0.1)\n",
    "train_dataset, update_dataset, test1_dataset, test2_dataset = transform_dataset(dataset=original_dataset,\n",
    "                                                                                test_fraction=test_fraction,\n",
    "                                                                                transform_name=config['transform_name'],\n",
    "                                                                                transform_parameters=config[\n",
    "                                                                                    'transform_parameters'],\n",
    "                                                                                seed=config['seed'])\n",
    "baseline_dataset = pd.concat([train_dataset, update_dataset])\n",
    "test_dataset = pd.concat([test1_dataset, test2_dataset])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "baseline_model = train(config['dataset_name'], baseline_dataset, verbose=verbose)\n",
    "elapsed_time = baseline_model.wait_to_complete()\n",
    "logger.info('Elapsed time: {}'.format(elapsed_time))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I] incremental_learning >> Elapsed time: 176.3931188583374\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "trained_model = train(dataset_name, train_dataset, verbose=verbose)\n",
    "elapsed_time = trained_model.wait_to_complete()\n",
    "logger.info('Elapsed time: {}'.format(elapsed_time))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I] incremental_learning >> Elapsed time: 126.05649495124817\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "updated_model = update(dataset_name, update_dataset, trained_model, force=force_update, verbose=verbose)\n",
    "elapsed_time = updated_model.wait_to_complete()\n",
    "logger.info('Elapsed time: {}'.format(elapsed_time))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[I] incremental_learning >> Elapsed time: 5.320300340652466\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "baseline_eval = evaluate(dataset_name, test_dataset, baseline_model, verbose=verbose)\n",
    "baseline_eval.wait_to_complete()\n",
    "\n",
    "trained_model_eval = evaluate(dataset_name, test_dataset, trained_model, verbose=verbose)\n",
    "trained_model_eval.wait_to_complete()\n",
    "\n",
    "updated_model_eval = evaluate(dataset_name, test_dataset, updated_model, verbose=verbose)\n",
    "updated_model_eval.wait_to_complete()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6.257479667663574"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "dependent_variable = baseline_model.dependent_variable\n",
    "\n",
    "scores = {}\n",
    "\n",
    "if baseline_model.is_regression():\n",
    "    y_true = np.array([y for y in test_dataset[dependent_variable]])\n",
    "    scores = compute_regression_metrics(y_true,\n",
    "                                        baseline_eval.get_predictions(),\n",
    "                                        trained_model_eval.get_predictions(),\n",
    "                                        updated_model_eval.get_predictions())\n",
    "elif baseline_model.is_classification():\n",
    "    y_true = np.array([str(y) for y in test_dataset[dependent_variable]])\n",
    "    scores = compute_classification_metrics(y_true,\n",
    "                                            baseline_eval.get_predictions(),\n",
    "                                            trained_model_eval.get_predictions(),\n",
    "                                            updated_model_eval.get_predictions())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "pprint.pprint(scores)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'baseline': {'mae': 3.4624664751688647, 'mse': 21.21438482287693},\n",
      " 'trained_model': {'mae': 4.137316767374676, 'mse': 29.138932961441892},\n",
      " 'updated_model': {'mae': 3.6626670138041177, 'mse': 23.4148928431643}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results show that the generalization error of the `updated_model` is lower than the generalization error of the `trained_model` and is much closer to the generalization error of the `baseline_model`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# path = jobs_dir/'demo_baseline_model'\n",
    "# baseline_model.store(destination=path)\n",
    "# success = upload_job(local_job_path=path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# delete_job('demo_baseline_model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd811bcf3f365bc5382730ef97b333cd6ca82417629bc5a6815cfad2f5503789"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}